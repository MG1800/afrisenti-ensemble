{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n# #         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# !rm -rf outputs/*\nlangs = ['am', 'dz', 'ha', 'ig', 'ma', 'pcm', 'pt', 'sw', 'yo', 'twi', 'kr', 'ts', 'multilingual']\n# LANG = 'multilingual'\n\nfrom sklearn.metrics import f1_score, precision_score, recall_score\n\nPATH = '/kaggle/working/full_outputs/'\nif not os.path.exists(PATH):\n    os.makedirs(PATH)\n\ndef saver(model, split, scores, pred, orig_df):\n    \n    save_dir = f\"{PATH}/{model}\"\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    \n    df = orig_df.copy()\n    \n    df['pred'] = pred\n    scores_df = pd.DataFrame(scores, columns=['negative', 'neutral', 'positive'])\n    df['negative'] = scores_df['negative']\n    df['neutral'] = scores_df['neutral']\n    df['positive'] = scores_df['positive']\n    \n    df.to_csv(f\"{save_dir}/{LANG}_{split}_output.tsv\", sep='\\t', index = False)\n    \n    if split != 'test':\n        f1 = f1_score(df['label'],df['pred'], average=\"weighted\")\n        recall = recall_score(df['label'], df['pred'], average=\"weighted\")\n        precision = precision_score(df['label'], df['pred'], average=\"weighted\")\n\n        m_f1 = f1_score(df['label'],df['pred'], average=\"macro\")\n        m_recall = recall_score(df['label'], df['pred'], average=\"macro\")\n        m_precision = precision_score(df['label'], df['pred'], average=\"macro\")\n        \n        with open(f\"{save_dir}/{LANG}_{split}_scores.txt\", 'w') as output_file:\n            output_file.write(\"WEIGHTED F1\\n\\navg_precision:{}\\n\".format(precision)) \n            output_file.write(\"avg_recall:{}\\n\".format(recall))\n            output_file.write(\"avg_f1_score:{}\\n\".format(f1))\n\n            output_file.write(\"\\nMACRO F1\\n\\nmacro_precision:{}\\n\".format(m_precision))\n            output_file.write(\"macro_recall:{}\\n\".format(m_recall))\n            output_file.write(\"macro_f1_score:{}\\n\".format(m_f1))\n        \nfor LANG in langs:\n    print(f'\\n{LANG}')\n    train_df = pd.read_csv(f'/kaggle/input/semevalafrisenti/comb_data/train/{LANG}_train.tsv', sep = '\\t')\n    dev_df = pd.read_csv(f'/kaggle/input/semevalafrisenti/comb_data/dev/{LANG}_dev.tsv', sep = '\\t')\n    test_df = pd.read_csv(f'/kaggle/input/semevalafrisenti/comb_data/test/{LANG}_test.tsv', sep = '\\t')\n\n    x_train, x_test, y_train, y_test = train_df['tweet'], dev_df['tweet'], train_df['label'], dev_df['label']\n    print(len(x_train),len(y_train),len(x_test),len(y_test))\n\n    x_unk = test_df['tweet']\n\n\n    #     return df\n\n    print('svm')\n\n    from sklearn.feature_extraction.text import TfidfVectorizer\n\n    vectorizer = TfidfVectorizer()\n\n    tf_x_train = vectorizer.fit_transform(x_train)\n    tf_x_test = vectorizer.transform(x_test)\n    tf_x_unk = vectorizer.transform(x_unk)\n\n    from sklearn.svm import LinearSVC\n    from sklearn.calibration import CalibratedClassifierCV\n\n    svc = LinearSVC(random_state = 0)\n    clf = CalibratedClassifierCV(svc) \n    # clf = LinearSVC(random_state = 0)\n\n    clf.fit(tf_x_train, y_train)\n\n    train_pred = clf.predict(tf_x_train)\n    train_scores = clf.predict_proba(tf_x_train)\n    saver(model = 'svm', split = 'train', scores = train_scores, pred = train_pred, orig_df = train_df)\n\n    dev_pred = clf.predict(tf_x_test)\n    dev_scores = clf.predict_proba(tf_x_test)\n    saver(model = 'svm', split = 'dev', scores = dev_scores, pred = dev_pred, orig_df = dev_df)\n\n    test_pred = clf.predict(tf_x_unk)\n    test_scores = clf.predict_proba(tf_x_unk)\n    saver(model = 'svm', split = 'test', scores = test_scores, pred = test_pred, orig_df = test_df)\n\n    # train_pred, train_scores\n\n#     !ls outputs/svm\n    print('LR')\n\n    from sklearn.linear_model import LogisticRegression\n\n    clf = LogisticRegression(max_iter = 1000)\n\n    clf.fit(tf_x_train, y_train)\n\n    train_pred = clf.predict(tf_x_train)\n    train_scores = clf.predict_proba(tf_x_train)\n    saver(model = 'logistic_regression', split = 'train', scores = train_scores, pred = train_pred, orig_df = train_df)\n\n    dev_pred = clf.predict(tf_x_test)\n    dev_scores = clf.predict_proba(tf_x_test)\n    saver(model = 'logistic_regression', split = 'dev', scores = dev_scores, pred = dev_pred, orig_df = dev_df)\n\n    test_pred = clf.predict(tf_x_unk)\n    test_scores = clf.predict_proba(tf_x_unk)\n    saver(model = 'logistic_regression', split = 'test', scores = test_scores, pred = test_pred, orig_df = test_df)\n\n\n#     !ls outputs/logistic_regression\n    print('RF')\n\n    from sklearn.ensemble import RandomForestClassifier\n\n    clf = RandomForestClassifier()\n\n    clf.fit(tf_x_train, y_train)\n\n    train_pred = clf.predict(tf_x_train)\n    train_scores = clf.predict_proba(tf_x_train)\n    saver(model = 'random_forest', split = 'train', scores = train_scores, pred = train_pred, orig_df = train_df)\n\n    dev_pred = clf.predict(tf_x_test)\n    dev_scores = clf.predict_proba(tf_x_test)\n    saver(model = 'random_forest', split = 'dev', scores = dev_scores, pred = dev_pred, orig_df = dev_df)\n\n    test_pred = clf.predict(tf_x_unk)\n    test_scores = clf.predict_proba(tf_x_unk)\n    saver(model = 'random_forest', split = 'test', scores = test_scores, pred = test_pred, orig_df = test_df)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-29T15:09:55.317436Z","iopub.execute_input":"2023-01-29T15:09:55.317840Z","iopub.status.idle":"2023-01-29T15:31:43.112215Z","shell.execute_reply.started":"2023-01-29T15:09:55.317804Z","shell.execute_reply":"2023-01-29T15:31:43.110874Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\nam\n5984 5984 1497 1497\nsvm\nLR\nRF\n\ndz\n1651 1651 414 414\nsvm\nLR\nRF\n\nha\n14172 14172 2677 2677\nsvm\nLR\nRF\n\nig\n10192 10192 1841 1841\nsvm\nLR\nRF\n\nma\n5583 5583 1215 1215\nsvm\nLR\nRF\n\npcm\n5121 5121 1281 1281\nsvm\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"LR\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"RF\n\npt\n3063 3063 767 767\nsvm\nLR\nRF\n\nsw\n1810 1810 453 453\nsvm\nLR\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"RF\n\nyo\n8522 8522 2090 2090\nsvm\nLR\nRF\n\ntwi\n3481 3481 388 388\nsvm\nLR\nRF\n\nkr\n3302 3302 827 827\nsvm\nLR\nRF\n\nts\n804 804 203 203\nsvm\nLR\nRF\n\nmultilingual\n63685 63685 13653 13653\nsvm\nLR\nRF\n","output_type":"stream"}]},{"cell_type":"code","source":"# from sklearn.neural_network import MLPClassifier\n\n# clf = MLPClassifier(hidden_layer_sizes=(100), max_iter=100, verbose=True, activation = 'relu',solver='adam',random_state=1)\n\n\n\n# clf.fit(tf_x_train, y_train)\n\n# train_pred = clf.predict(tf_x_train)\n# train_scores = clf.predict_proba(tf_x_train)\n# saver(model = 'mlp', split = 'train', scores = train_scores, pred = train_pred, orig_df = train_df)\n\n# dev_pred = clf.predict(tf_x_test)\n# dev_scores = clf.predict_proba(tf_x_test)\n# saver(model = 'mlp', split = 'dev', scores = dev_scores, pred = dev_pred, orig_df = dev_df)\n\n# test_pred = clf.predict(tf_x_unk)\n# test_scores = clf.predict_proba(tf_x_unk)\n# saver(model = 'mlp', split = 'test', scores = test_scores, pred = test_pred, orig_df = test_df)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-01-29T15:31:43.114632Z","iopub.execute_input":"2023-01-29T15:31:43.115047Z","iopub.status.idle":"2023-01-29T15:31:43.120659Z","shell.execute_reply.started":"2023-01-29T15:31:43.115009Z","shell.execute_reply":"2023-01-29T15:31:43.119528Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import FileLink\n\n# !tar -czvf combined_output_with_test.tar.gz outputs\n# FileLink(r'combined_output_with_test.tar.gz')","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-01-29T15:31:43.122309Z","iopub.execute_input":"2023-01-29T15:31:43.123010Z","iopub.status.idle":"2023-01-29T15:31:43.134739Z","shell.execute_reply.started":"2023-01-29T15:31:43.122972Z","shell.execute_reply":"2023-01-29T15:31:43.133811Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# !cat outputs/random_forest/*dev*.txt","metadata":{"execution":{"iopub.status.busy":"2023-01-29T15:31:43.137665Z","iopub.execute_input":"2023-01-29T15:31:43.138582Z","iopub.status.idle":"2023-01-29T15:31:43.145724Z","shell.execute_reply.started":"2023-01-29T15:31:43.138495Z","shell.execute_reply":"2023-01-29T15:31:43.144690Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# import emoji","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-01-29T15:31:43.147133Z","iopub.execute_input":"2023-01-29T15:31:43.147489Z","iopub.status.idle":"2023-01-29T15:31:43.156946Z","shell.execute_reply.started":"2023-01-29T15:31:43.147458Z","shell.execute_reply":"2023-01-29T15:31:43.156070Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# output_dir = \"/kaggle/working/outputs/formatted_data/er\"\n\n# if not os.path.exists(output_dir):\n#     os.makedirs(output_dir)\n\n# new_train_df = train_df.copy()\n# new_train_df['tweet'] = train_df['tweet'].apply(lambda x: emoji.demojize(x, delimiters=(\"\", \"\")))\n# new_train_df.to_csv(f\"{output_dir}/ha_train_er.tsv\", sep = '\\t', index = False)\n\n# new_dev_df = dev_df.copy()\n# new_dev_df['tweet'] = dev_df['tweet'].apply(lambda x: emoji.demojize(x, delimiters=(\"\", \"\")))\n# new_dev_df.to_csv(f\"{output_dir}/ha_dev_er.tsv\", sep = '\\t', index = False)\n\n# new_test_df = test_df.copy()\n# new_test_df['tweet'] = test_df['tweet'].apply(lambda x: emoji.demojize(x, delimiters=(\"\", \"\")))\n# new_test_df.to_csv(f\"{output_dir}/ha_test_er.tsv\", sep = '\\t', index = False)\n\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-01-29T15:31:43.158022Z","iopub.execute_input":"2023-01-29T15:31:43.159096Z","iopub.status.idle":"2023-01-29T15:31:43.167536Z","shell.execute_reply.started":"2023-01-29T15:31:43.159062Z","shell.execute_reply":"2023-01-29T15:31:43.166535Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# print(new_train_df['tweet'][0])","metadata":{"execution":{"iopub.status.busy":"2023-01-29T15:31:43.169015Z","iopub.execute_input":"2023-01-29T15:31:43.169721Z","iopub.status.idle":"2023-01-29T15:31:43.179254Z","shell.execute_reply.started":"2023-01-29T15:31:43.169684Z","shell.execute_reply":"2023-01-29T15:31:43.177899Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# import re\n# def cleaner(tweet):\n#     whitespace = re.compile(r\"\\s+\")\n#     web_address = re.compile(r\"(?i)http(s):\\/\\/[a-z0-9.~_\\-\\/]+\")\n#     user = re.compile(r\"(?i)@[a-z0-9_]+\")\n    \n# #     tweet = text.replace(\"@user\", \"\")\n#     tweet = whitespace.sub(' ', tweet)\n#     tweet = web_address.sub('', tweet)\n#     tweet = user.sub('', tweet)\n#     return tweet","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-01-29T15:31:43.181041Z","iopub.execute_input":"2023-01-29T15:31:43.181665Z","iopub.status.idle":"2023-01-29T15:31:43.189704Z","shell.execute_reply.started":"2023-01-29T15:31:43.181621Z","shell.execute_reply":"2023-01-29T15:31:43.188542Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# output_dir = \"/kaggle/working/outputs/formatted_data/er_clean\"\n\n# if not os.path.exists(output_dir):\n#     os.makedirs(output_dir)\n\n# new_train_df = train_df.copy()\n# new_train_df['tweet'] = train_df['tweet'].apply(lambda x: cleaner(x))\n# new_train_df['tweet'] = train_df['tweet'].apply(lambda x: emoji.demojize(x, delimiters=(\"\", \"\")))\n# new_train_df.to_csv(f\"{output_dir}/ha_train_er_clean.tsv\", sep = '\\t', index = False)\n\n# new_dev_df = dev_df.copy()\n# new_dev_df['tweet'] = dev_df['tweet'].apply(lambda x: cleaner(x))\n# # new_dev_df['tweet'] = dev_df['tweet'].apply(lambda x: emoji.demojize(x, delimiters=(\"\", \"\")))\n# new_dev_df.to_csv(f\"{output_dir}/ha_dev_er_clean.tsv\", sep = '\\t', index = False)\n\n# new_test_df = test_df.copy()\n# new_test_df['tweet'] = test_df['tweet'].apply(lambda x: cleaner(x))\n# # new_test_df['tweet'] = test_df['tweet'].apply(lambda x: emoji.demojize(x, delimiters=(\"\", \"\")))\n# new_test_df.to_csv(f\"{output_dir}/ha_test_er_clean.tsv\", sep = '\\t', index = False)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-01-29T15:31:43.192108Z","iopub.execute_input":"2023-01-29T15:31:43.192570Z","iopub.status.idle":"2023-01-29T15:31:43.206429Z","shell.execute_reply.started":"2023-01-29T15:31:43.192532Z","shell.execute_reply":"2023-01-29T15:31:43.205110Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"!ls full_outputs/*/* | wc -l","metadata":{"execution":{"iopub.status.busy":"2023-01-29T15:31:43.212277Z","iopub.execute_input":"2023-01-29T15:31:43.212774Z","iopub.status.idle":"2023-01-29T15:31:44.355205Z","shell.execute_reply.started":"2023-01-29T15:31:43.212730Z","shell.execute_reply":"2023-01-29T15:31:44.353671Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"195\n","output_type":"stream"}]},{"cell_type":"code","source":"# from IPython.display import FileLink\n\n# !tar -czvf modified_data.tar.gz outputs/formatted_data\n# FileLink(r'modified_data.tar.gz')","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-01-29T15:31:44.357861Z","iopub.execute_input":"2023-01-29T15:31:44.359749Z","iopub.status.idle":"2023-01-29T15:31:44.364998Z","shell.execute_reply.started":"2023-01-29T15:31:44.359643Z","shell.execute_reply":"2023-01-29T15:31:44.363712Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}